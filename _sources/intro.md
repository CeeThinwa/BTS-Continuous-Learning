# Behind the Scenes of my Learning Journey

This book is an effort for me to document my journey
as I learn different technologies and share both the
key ideas I found and the mistakes I made - so you
don't have to.

This book contains the following sections:

## 1. Jupyter Books explored

In this section, I literally take you through how I
learned to set this Jupyter Book up.

## 2. The NLP toolbox

This was created during my time as a 2021 Delta
Analytics Fellow. This framework really enabled me
to structure my NLP problem in an optimal way,
leading to my first YouTube lecture.

<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/2TUK9QytzFo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

BONUS! You get to also learn from my struggles in model
selection and optimization, working with JSON and
translating data between Python and R environments.


## 3. Streamlit rediscovered

I first came across Streamlit in a Data Umbrella webinar,
and it really piqued my interest. Fast-forward to my
fellowship a couple of months later, I got the
opportunity this time to learn it in a coding lab
organised by a Delta Analytics facilitator, Brian Spiering.

It was not only enjoyable, but also forced me to get
my hands dirty with it (cue an Aha! moment that I
learn best in a project-based doing style compared to
a webinar listening style).

This came in handy when I got the opportunity to use
it as a localised proof of concept screen-shared demo.
It really made the demo come alive, and I got to
receive great suggestions around my modelling approach
like considering a reinforcement learning based approach
and optimizing my code for speed.

So in this section, I aim to share what I learned
and learn additional information. A key goal I have around
Streamlit is learning how to customize the front-end
and see if I can incorporate transformers.


## 4. Graph Databases Relearned

I got to interact with graph technology for the
first time in a team at Bootcamp 33, and it was
pretty awesome.
In this section I aim to share strategies that helped
me understand this concept the first time round and
share my journey in recreating:

* how to curate data for a graph
* how to set up the schema for a graph (logic behind connections and visualization of the schema)
* how to write and run queries for a graph

I aim to share both platform-specific and
platform-agnostic solutions around graphs.

## 5. Building my first website

One of the things that I learned in my internship
experience at Adrian Ltd. was that machine learning
needs a home that has a nice and clean frontend -
Jupyter notebooks were not appreciated as much by
people who were not from the AI/ML community. A
popular solution to deploy models involved using
Flask. A colleague at the time mentioned Django as
it was more robust and secure.

When pursuing my studies and facing a slowing life
as a result of the pandemic, I saw building a
website for the first time as a way to keep busy,
learn web development and deployment and share my
Masters journey.

Here I share the challenges I faced
(especially interacting with Linux for the first time
and setting up an SSL certificate) and the lessons
I learned along the way.

## 6. Open-source data mining explored

Heavily inspired  by my marketing roots and a strong
desire to understand what skills were marketable for
a marketing researcher, I decided to mine job ads.
This was a project that really helped clarify my
career path and stretch myself beyond my comfort
zone.

In this section I share what it took for me to come
up with a data mining Jupyter notebook that worked.


## 7. Text analysis 101

The first unstructured data that I ventured to use
for a data science project was text.

In this section, I share what it took to come up with
the collection of notebooks. It was at this point that
I really got comfortable with Google Colab.

## 8. Behind my KIVA project

I had volunteered to mentor in a local initiative to
motivate Kenyans to master Data Science called Master
Cohorts. I decided to attempt the first assignment,
which was based on a past Kaggle competition aiming
to solve a problem for KIVA.

One of the highlights around working on this project
was that I really stretched my Pandas skills and also
got to marry two completely different datasets to
create the final notebook.

I share my Pandas journey in this section.
